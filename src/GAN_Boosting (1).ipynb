{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some parameters for training\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 30\n",
    "num_iterators = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "#Load in the data (If you want f)\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "#Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#Process\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the data (If you want undersampled dataset)\n",
    "undersample_path = '/home/ec2-user/SageMaker/efs/Data/UnderSamples/'\n",
    "(_,_), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = np.load(undersample_path + 'X9Prop0.1.npy')\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "y_train = np.load(undersample_path + 'Y9Prop0.1.npy')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Set up the model function\n",
    "def model_func():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # initiate RMSprop optimizer\n",
    "    opt = tensorflow.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "    \n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 11s 221us/sample - loss: 4.9186e-05 - acc: 0.1527 - val_loss: 2.0741 - val_acc: 0.2485\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 4.4997e-05 - acc: 0.2437 - val_loss: 1.9792 - val_acc: 0.2838\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 4.3415e-05 - acc: 0.2798 - val_loss: 1.9016 - val_acc: 0.3122\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 4.1005e-05 - acc: 0.3281 - val_loss: 1.7752 - val_acc: 0.3679\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 3.8883e-05 - acc: 0.3655 - val_loss: 1.7029 - val_acc: 0.3958\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 3.7220e-05 - acc: 0.3931 - val_loss: 1.6761 - val_acc: 0.4004\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 3.6047e-05 - acc: 0.4108 - val_loss: 1.6135 - val_acc: 0.4179\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 3.4895e-05 - acc: 0.4338 - val_loss: 1.6230 - val_acc: 0.4213\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 3.4123e-05 - acc: 0.4416 - val_loss: 1.6110 - val_acc: 0.4259\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 3.3274e-05 - acc: 0.4600 - val_loss: 1.5173 - val_acc: 0.4552\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 3.2566e-05 - acc: 0.4682 - val_loss: 1.5555 - val_acc: 0.4482\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 3.1882e-05 - acc: 0.4802 - val_loss: 1.4961 - val_acc: 0.4683\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 3.1284e-05 - acc: 0.4922 - val_loss: 1.4482 - val_acc: 0.4815\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 3.0735e-05 - acc: 0.4995 - val_loss: 1.4663 - val_acc: 0.4783\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 3.0243e-05 - acc: 0.5063 - val_loss: 1.4146 - val_acc: 0.4914\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 2.9798e-05 - acc: 0.5155 - val_loss: 1.4135 - val_acc: 0.4991\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 2.9388e-05 - acc: 0.5228 - val_loss: 1.4004 - val_acc: 0.5072\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 2.8815e-05 - acc: 0.5330 - val_loss: 1.3629 - val_acc: 0.5134\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 2.8372e-05 - acc: 0.5424 - val_loss: 1.3469 - val_acc: 0.5230\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 2.7912e-05 - acc: 0.5492 - val_loss: 1.3222 - val_acc: 0.5329\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 2.7578e-05 - acc: 0.5540 - val_loss: 1.3153 - val_acc: 0.5357\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 10s 203us/sample - loss: 2.7232e-05 - acc: 0.5633 - val_loss: 1.2976 - val_acc: 0.5423\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 2.6900e-05 - acc: 0.5700 - val_loss: 1.3029 - val_acc: 0.5416\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 2.6511e-05 - acc: 0.5742 - val_loss: 1.2972 - val_acc: 0.5481\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 2.6055e-05 - acc: 0.5815 - val_loss: 1.2879 - val_acc: 0.5546\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 10s 203us/sample - loss: 2.5757e-05 - acc: 0.5891 - val_loss: 1.2953 - val_acc: 0.5492\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 2.5541e-05 - acc: 0.5913 - val_loss: 1.2387 - val_acc: 0.5666\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 2.5194e-05 - acc: 0.5965 - val_loss: 1.2210 - val_acc: 0.5758\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 2.4824e-05 - acc: 0.6043 - val_loss: 1.2286 - val_acc: 0.5771\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 10s 203us/sample - loss: 2.4635e-05 - acc: 0.6104 - val_loss: 1.2170 - val_acc: 0.5780\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 11s 219us/sample - loss: 4.9591e-05 - acc: 0.1084 - val_loss: 2.2946 - val_acc: 0.1095\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 4.8051e-05 - acc: 0.1569 - val_loss: 2.1222 - val_acc: 0.1929\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 4.6372e-05 - acc: 0.2011 - val_loss: 2.0744 - val_acc: 0.2116\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 4.5337e-05 - acc: 0.2288 - val_loss: 2.0245 - val_acc: 0.2219\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 4.4546e-05 - acc: 0.2430 - val_loss: 1.9778 - val_acc: 0.2430\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 4.3943e-05 - acc: 0.2584 - val_loss: 1.9301 - val_acc: 0.2598\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 4.3403e-05 - acc: 0.2682 - val_loss: 1.8813 - val_acc: 0.2832\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 4.2891e-05 - acc: 0.2769 - val_loss: 1.8647 - val_acc: 0.2921\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 4.2390e-05 - acc: 0.2889 - val_loss: 1.8665 - val_acc: 0.2776\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 4.1906e-05 - acc: 0.3011 - val_loss: 1.8136 - val_acc: 0.2907\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 4.1361e-05 - acc: 0.3129 - val_loss: 1.7811 - val_acc: 0.3109\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 4.0959e-05 - acc: 0.3195 - val_loss: 1.7532 - val_acc: 0.3225\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 4.0639e-05 - acc: 0.3270 - val_loss: 1.7956 - val_acc: 0.3077\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 4.0427e-05 - acc: 0.3346 - val_loss: 1.7921 - val_acc: 0.3060\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 4.0162e-05 - acc: 0.3426 - val_loss: 1.7443 - val_acc: 0.3272\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 3.9952e-05 - acc: 0.3459 - val_loss: 1.7383 - val_acc: 0.3195\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 10s 199us/sample - loss: 3.9611e-05 - acc: 0.3548 - val_loss: 1.7633 - val_acc: 0.3298\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 3.9367e-05 - acc: 0.3583 - val_loss: 1.7089 - val_acc: 0.3347\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 3.9180e-05 - acc: 0.3637 - val_loss: 1.7086 - val_acc: 0.3338\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 10s 201us/sample - loss: 3.9014e-05 - acc: 0.3656 - val_loss: 1.6786 - val_acc: 0.3646\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 3.8771e-05 - acc: 0.3715 - val_loss: 1.6894 - val_acc: 0.3526\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 3.8545e-05 - acc: 0.3781 - val_loss: 1.6908 - val_acc: 0.3451\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 3.8405e-05 - acc: 0.3814 - val_loss: 1.6634 - val_acc: 0.3601\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 10s 199us/sample - loss: 3.8170e-05 - acc: 0.3876 - val_loss: 1.6678 - val_acc: 0.3628\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 10s 200us/sample - loss: 3.8012e-05 - acc: 0.3899 - val_loss: 1.6281 - val_acc: 0.3758\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49792/50000 [============================>.] - ETA: 0s - loss: 3.7771e-05 - acc: 0.3946"
     ]
    }
   ],
   "source": [
    "model_store = []\n",
    "history_store = []\n",
    "alpha_store = []\n",
    "weight_store = [np.array(x_train.shape[0]*[1/x_train.shape[0]])] #Initialize as 1/n for all\n",
    "performance_matrix = np.zeros((num_classes,num_iterators)) #Accuracy of nth model on mth class\n",
    "\n",
    "for i in range(num_iterators):\n",
    "    #GAN\n",
    "    from_idx = i * 4500\n",
    "    to_idx = (i+1) * 4500\n",
    "    gan_fakes = fakes[from_idx:to_idx]\n",
    "    \n",
    "    #augment x\n",
    "    augmented_x_train = np.concatenate((x_train,gan_fakes),axis=0)\n",
    "    augmented_y_train = np.concatenate((np.argmax(y_train,axis=1), np.array(4500*[9])))\n",
    "    augmented_y_train = keras.utils.to_categorical(augmented_y_train, num_classes)\n",
    "    augmented_weight_store = np.concatenate((weight_store[i],np.array(4500*[np.mean(weight_store[i])])))\n",
    "    \n",
    "    #Create current model, train it, store it and it's history.\n",
    "    curr_model = model_func()\n",
    "    curr_history = curr_model.fit(augmented_x_train, augmented_y_train, batch_size=batch_size, epochs=epochs, sample_weight=augmented_weight_store, validation_data=(x_test, y_test), shuffle=True)\n",
    "    model_store.append(curr_model)\n",
    "    history_store.append(curr_history)\n",
    "    \n",
    "    #Recalculate the weights, update weights\n",
    "    prediction = curr_model.predict(x_train) #predict    \n",
    "    predict_matrix = (prediction == prediction.max(axis=1)[:,None]).astype(int) #Convert to 1-0 matrix\n",
    "    match_matrix = y_train - predict_matrix #Difference to eventually generate match_vector\n",
    "    match_vector = np.sum(np.abs(match_matrix), axis = 1)/2 #1 indicates no match\n",
    "    error = np.sum(weight_store[i] * match_vector)/np.sum(weight_store[i]) #Calculate error\n",
    "    alpha = np.log((1-error)/error) + np.log(num_classes-1) #Calulate alpha\n",
    "    alpha_store.append(alpha) #Append\n",
    "    new_weight = weight_store[i] * np.exp(alpha * match_vector) #Calculate new weights\n",
    "    new_weight /= np.sum(new_weight) #Normalize\n",
    "    weight_store.append(new_weight) #Append\n",
    "    \n",
    "    #Update our performance matrix\n",
    "    prediction_vector = np.argmax(prediction, axis = 1)\n",
    "    actual_vector = np.argmax(y_train, axis = 1)\n",
    "    for j in range(len(actual_vector)):\n",
    "        if actual_vector[j]==prediction_vector[j]:\n",
    "            performance_matrix[actual_vector[j]][i] += 1\n",
    "            \n",
    "    if i%10==0 or i == num_iterators-1:\n",
    "        np.save('/home/ec2-user/SageMaker/efs/Data/GANBoostStore/Weights/new_weight_re_GB' + str(i) + '.npy', new_weight)\n",
    "        np.save('/home/ec2-user/SageMaker/efs/Data/GANBoostStore/Alpha/alpha_re_GB' + str(i) + '.npy', alpha_store)\n",
    "        curr_model.save_weights('/home/ec2-user/SageMaker/efs/Data/GANBoostStore/Model/model_re_GB' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change performance matrix to accuracy figures:\n",
    "normed_perf_matrix = np.zeros_like(performance_matrix)\n",
    "\n",
    "for i in range(10):\n",
    "    i_count = list(np.argmax(y_train,axis=1)).count(i)\n",
    "    normed_perf_matrix[i] = performance_matrix[i]/i_count\n",
    "\n",
    "#Normalize for algorithm:\n",
    "normed_perf_matrix /= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the final prediction\n",
    "final_matrix = np.ones((len(x_test), num_classes))\n",
    "\n",
    "for i in range(len(model_store)):\n",
    "    prediction_vector = np.argmax(model_store[i].predict(x_test), axis = 1)\n",
    "    for j in range(len(prediction_vector)):\n",
    "        final_matrix[j][prediction_vector[j]] = final_matrix[j][prediction_vector[j]] * normed_perf_matrix[prediction_vector[j]][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
