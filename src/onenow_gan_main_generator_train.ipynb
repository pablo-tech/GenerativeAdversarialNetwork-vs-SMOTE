{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is the main code for the General Adversarial Network\\nTrains GAN as specified in the /architecture folder\\nSearches for best fit hyper-parameters \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the main code for the General Adversarial Network\n",
    "Trains GAN as specified in the /architecture folder\n",
    "Searches for best fit hyper-parameters \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========\n",
    "# TODO:\n",
    "# network visualiztion https://lutzroeder.github.io/netron/\n",
    "# any object dataset\n",
    "# assert shape\n",
    "# shift tab ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Library: general\n",
    "\"\"\"\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Library: randomness\n",
    "\"\"\"\n",
    "import random\n",
    "import scipy \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Result repdoducibility\n",
    "\"\"\"\n",
    "RANDOM_SEED = 0\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIPY= 1.3.1\n",
      "NUMPY= 1.16.4\n",
      "TENSORFLOW= 1.14.0\n",
      "KERAS= 2.2.4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Versioning\n",
    "\"\"\"\n",
    "print(\"SCIPY=\", scipy.__version__)\n",
    "print(\"NUMPY=\", np.__version__)\n",
    "print(\"TENSORFLOW=\", tf.__version__)\n",
    "print(\"KERAS=\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwww.wandb.com experiment tracking installation \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "www.wandb.com experiment tracking installation \n",
    "\"\"\"\n",
    "\n",
    "# !pip install graphql-core==2.0\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\r\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "www.wandb.com experiment tracking loging \n",
    "\"\"\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "!wandb login 45396bf25753eeeb051e5567c5e7dd67446e3be4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlotting dependencies\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plotting dependencies\n",
    "\"\"\"\n",
    "\n",
    "# !pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load architecture for Discriminator and Generator\n",
    "\"\"\"\n",
    "from architecture import atienza_G as atienza_G\n",
    "from architecture import atienza_D as atienza_D\n",
    "from architecture import atienza_bertorello_G as atiber_G\n",
    "from architecture import atienza_bertorello_D as atiber_D\n",
    "from architecture import brownlee_G as brownlee_G\n",
    "from architecture import brownlee_D as brownlee_D\n",
    "\n",
    "\n",
    "def get_G(architecture_choice, input_n):\n",
    "    if architecture_choice == 'atienza':\n",
    "        return atienza_G.DifferetiableNetwork(input_n)\n",
    "    if architecture_choice == 'atiber':\n",
    "        return atiber_G.DifferetiableNetwork(input_n)\n",
    "    if architecture_choice == 'brownlee':\n",
    "        return brownlee_G.DifferetiableNetwork(input_n)\n",
    "        \n",
    "def get_D(architecture_choice, object_shape):    \n",
    "    if architecture_choice == 'atienza':\n",
    "        return atienza_D.DifferetiableNetwork(object_shape)\n",
    "    if architecture_choice == 'atiber':\n",
    "        return atiber_D.DifferetiableNetwork(object_shape)\n",
    "    if architecture_choice == 'brownlee':\n",
    "        return brownlee_D.DifferetiableNetwork(object_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss functions\n",
    "https://github.com/TwistedW/tf-GANs-Loss/blob/master/GAN_Loss.py\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tf_contrib\n",
    "\n",
    "def discriminator_loss(loss_func, real, fake):\n",
    "    real_loss = 0\n",
    "    fake_loss = 0\n",
    "\n",
    "    if loss_func.__contains__('wgan'):\n",
    "        real_loss = -tf.reduce_mean(real)\n",
    "        fake_loss = tf.reduce_mean(fake)\n",
    "\n",
    "    if loss_func == 'lsgan':\n",
    "        real_loss = tf.reduce_mean(tf.squared_difference(real, 1.0))\n",
    "        fake_loss = tf.reduce_mean(tf.square(fake))\n",
    "\n",
    "    if loss_func == 'gan' or loss_func == 'dragan':\n",
    "        real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real), logits=real))\n",
    "        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake), logits=fake))\n",
    "\n",
    "    if loss_func == 'hinge':\n",
    "        real_loss = tf.reduce_mean(relu(1.0 - real))\n",
    "        fake_loss = tf.reduce_mean(relu(1.0 + fake))\n",
    "\n",
    "    loss = real_loss + fake_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def generator_loss(loss_func, fake):\n",
    "    fake_loss = 0\n",
    "\n",
    "    if loss_func.__contains__('wgan'):\n",
    "        fake_loss = -tf.reduce_mean(fake)\n",
    "\n",
    "    if loss_func == 'lsgan':\n",
    "        fake_loss = tf.reduce_mean(tf.squared_difference(fake, 1.0))\n",
    "\n",
    "    if loss_func == 'gan' or loss_func == 'dragan':\n",
    "        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake), logits=fake))\n",
    "\n",
    "    if loss_func == 'hinge':\n",
    "        fake_loss = -tf.reduce_mean(fake)\n",
    "\n",
    "    loss = fake_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "def gradient_penalty(self, real, fake):\n",
    "    if self.gan_type == 'dragan':\n",
    "        shape = tf.shape(real)\n",
    "        eps = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n",
    "        x_mean, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3])\n",
    "        x_std = tf.sqrt(x_var)  # magnitude of noise decides the size of local region\n",
    "        noise = 0.5 * x_std * eps  # delta in paper\n",
    "\n",
    "        # Author suggested U[0,1] in original paper, but he admitted it is bug in github\n",
    "        # (https://github.com/kodalinaveen3/DRAGAN). It should be two-sided.\n",
    "\n",
    "        alpha = tf.random_uniform(shape=[shape[0], 1, 1, 1], minval=-1., maxval=1.)\n",
    "        interpolated = tf.clip_by_value(real + alpha * noise, -1., 1.)  # x_hat should be in the space of X\n",
    "\n",
    "    else:\n",
    "        alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\n",
    "        interpolated = alpha * real + (1. - alpha) * fake\n",
    "\n",
    "    logit = self.discriminator(interpolated, reuse=True)\n",
    "\n",
    "    grad = tf.gradients(logit, interpolated)[0]  # gradient of D(interpolated)\n",
    "    grad_norm = tf.norm(flatten(grad), axis=1)  # l2 norm\n",
    "\n",
    "    GP = 0\n",
    "\n",
    "    # WGAN - LP\n",
    "    if self.gan_type == 'wgan-lp':\n",
    "        GP = self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.)))\n",
    "\n",
    "    elif self.gan_type == 'wgan-gp' or self.gan_type == 'dragan':\n",
    "        GP = self.ld * tf.reduce_mean(tf.square(grad_norm - 1.))\n",
    "\n",
    "    return GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA note on training GANs\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A note on training GANs\n",
    "\"\"\"\n",
    "# https://github.com/eriklindernoren/Keras-GAN\n",
    "# https://www.youtube.com/watch?v=XeQBsidyhWE\n",
    "# https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy\n",
    "# https://machinelearningmastery.com/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions/\n",
    "# https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\n",
    "\n",
    "# def CrossEntropyLoss(yHat, y):\n",
    "#     if y == 1:\n",
    "#       return -log(yHat)       // loss rapidly increasing with yHat -> 0\n",
    "#     else:\n",
    "#       return -log(1-yHat)     // loss rapidly increasing with yHat -> 1\n",
    "\n",
    "# D is trained with binary_crossentropy:\n",
    "# a) true x examples with label y=1, and thus yHat=D(x)\n",
    "# b) fake G(z) examples with label y=0, and thus yHat=D(G(z))\n",
    "# Thus D loss in heavyside form:\n",
    "# Example loss = yTrue*log(D(x)) + yGen*log(1−D(G(z)))\n",
    "# minimize J = - Sum(loss) / m\n",
    "\n",
    "# GD is trained with binary_crossentropy (to fool D):\n",
    "# a) with discriminator fake label y=1, and thus yHat=D(G(z))\n",
    "# b) no training with label y=0\n",
    "# Thus G loss is defined heavyside: \n",
    "# Example loss = 0 + yGen*log(1-D(G(z))) \n",
    "# minimize J = Sum(loss) / m\n",
    "\n",
    "# For faster training of D, with stronger gradients: \n",
    "# Example loss = log(D(G(z)))\n",
    "# J = - Sum(loss) / m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKeras custom loss definition example\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keras custom loss definition example\n",
    "\"\"\"\n",
    "# # Define custom loss\n",
    "# def custom_loss(layer):\n",
    "\n",
    "#     # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
    "#     def strong_gradient_cross_entropy_loss(y_true, y_pred):\n",
    "#         return K.mean(K.square(y_pred - y_true) + K.square(layer), axis=-1)\n",
    "   \n",
    "#     return strong_gradient_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assemble a GAN from: Discriminator, Generator and other parameters\n",
    "\"\"\"\n",
    "import onenow_gan_factory_adversarial as adversarial_factory\n",
    "import onenow_gan_factory_sequential as sequential_factory\n",
    "\n",
    "    \n",
    "def get_adversarial_net(architecture_choice, optimizer_type, learning_rate, \n",
    "                        object_shape, G_input_n, net_config,\n",
    "                        transfer_D_path, transfer_GD_path,\n",
    "                        D_num_frozen_layers, GD_num_frozen_layers):\n",
    "    \"\"\"\n",
    "    D is trained to: a) correctly label real example as 1, and b) correctly label fake example as 0\n",
    "    Success for G succeeds fooling D, generating fake images indistiguishable from real ones\n",
    "    Success for D is detecting fakes; ultimately accuracy converges to 50% (fakes as good as real) \n",
    "    \"\"\"        \n",
    "    # binary cross-entropy loss \n",
    "    discriminator_loss_function = 'binary_crossentropy' # custom_loss\n",
    "    discriminator_learning_metrics = ['accuracy']\n",
    "\n",
    "    # optimizer: Generator\n",
    "    generator_loss_function = 'binary_crossentropy'\n",
    "    generator_learning_metrics = ['accuracy'] \n",
    "\n",
    "    G_net = get_G(architecture_choice, G_input_n) \n",
    "    D_net = get_D(architecture_choice, object_shape)\n",
    "                        \n",
    "    # model\n",
    "    # TODO: rename Sequential_Network to NetworkModel \n",
    "    discriminator_model = sequential_factory.Sequential_Network(\"D_\", [D_net], \n",
    "                optimizer_type, learning_rate, \n",
    "                discriminator_loss_function, discriminator_learning_metrics,\n",
    "                net_config['discriminator_model_path'], transfer_D_path, D_num_frozen_layers)\n",
    "    \n",
    "    # prevent discriminator from converging much faster than generator\n",
    "    generator_learning_fraction = 2 # k TODO: turn into hyper param, use to inner loop instead\n",
    "    generator_learning_rate = learning_rate / generator_learning_fraction\n",
    "    \n",
    "    generator_model = sequential_factory.Sequential_Network(\"GD_\", [G_net, D_net], \n",
    "                optimizer_type, generator_learning_rate, \n",
    "                generator_loss_function, generator_learning_metrics,\n",
    "                net_config['generator_model_path'], transfer_GD_path, GD_num_frozen_layers)\n",
    "\n",
    "    return adversarial_factory.Adversarial_Network(generator_model, discriminator_model), \\\n",
    "         discriminator_model, generator_model   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataset augmentation libarry\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset augmentation libarry\n",
    "\"\"\"\n",
    "# !pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aleju/imgaug\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "ia.seed(1)\n",
    "\n",
    "# https://imgaug.readthedocs.io/en/latest/source/examples_basics.html\n",
    "# It applies crops and affine transformations to images, flips some of the images horizontally, \n",
    "# adds a bit of noise and blur and also changes the contrast as well as brightness.\n",
    "\n",
    "crop_and_afine_transformation = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontal flips\n",
    "    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "    # But we only blur about 50% of all images.\n",
    "    iaa.Sometimes(0.5,\n",
    "        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "    ),\n",
    "    # Strengthen or weaken the contrast in each image.\n",
    "    iaa.LinearContrast((0.75, 1.5)),\n",
    "    # Add gaussian noise.\n",
    "    # For 50% of all images, we sample the noise once per pixel.\n",
    "    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "    # channel. This can change the color (not only brightness) of the\n",
    "    # pixels.\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "    # Make some images brighter and some darker.\n",
    "    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "    # which can end up changing the color of the images.\n",
    "    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "    # Apply affine transformations to each image.\n",
    "    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "        rotate=(-25, 25),\n",
    "        shear=(-8, 8)\n",
    "    )\n",
    "], random_order=True) # apply augmenters in random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_before_class_filter= (50000, 32, 32, 3)\n",
      "x_train_after_downsample= (500, 32, 32, 3)\n",
      "x_train_after_augmentation= (500, 32, 32, 3)\n",
      "x_train_after_class_filter= (500, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_before_class_filter= (50000, 32, 32, 3)\n",
      "x_train_after_downsample= (500, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_after_augmentation= (5500, 32, 32, 3)\n",
      "x_train_after_class_filter= (5500, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load a dataset: MNIST, CIFAR, etc\n",
    "\"\"\"\n",
    "import onenow_gan_utils as outil \n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "# from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets\n",
    "# https://github.com/astorfi/TensorFlow-World/tree/master/docs/tutorials/3-neural_network/autoencoder\n",
    "def get_data_set(dataset_name, class_list, is_augment, downsample_percent):\n",
    "    \n",
    "    ## dataset\n",
    "    if dataset_name == \"mnist\":\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    if dataset_name == \"cifar10\":\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()                \n",
    "\n",
    "    ## class: filter \n",
    "    print(\"x_train_before_class_filter=\", x_train.shape)        \n",
    "    train_filter = []\n",
    "    for i in range(len(x_train)):\n",
    "        if y_train[i] in class_list:\n",
    "            train_filter.append(i)\n",
    "    x_train = x_train[train_filter]            \n",
    "    y_train = y_train[train_filter] \n",
    "    \n",
    "    ## downsample\n",
    "    downsample_count = len(y_train) * downsample_percent \n",
    "    idx = np.random.choice(len(y_train), int(downsample_count))\n",
    "    x_train = x_train[idx]\n",
    "    print(\"x_train_after_downsample=\", x_train.shape)        \n",
    "\n",
    "    ## augment    \n",
    "    if is_augment:\n",
    "        x_all_augmented = x_train\n",
    "        for i in range(10): # augmenters are applied in random order\n",
    "            x_train_augmented = crop_and_afine_transformation(images=x_train)\n",
    "            x_all_augmented = np.concatenate((x_all_augmented , x_train_augmented), axis=0)\n",
    "        x_train = x_all_augmented    \n",
    "    print(\"x_train_after_augmentation=\", x_train.shape)\n",
    "\n",
    "    ## process: normalize\n",
    "    # convert from unsigned ints to floats\n",
    "    x_train = x_train.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    x_train = (x_train - 127.5) / 127.5\n",
    "    print(\"x_train_after_class_filter=\", x_train.shape)\n",
    "\n",
    "    return x_train, x_train[0].shape\n",
    "\n",
    "\n",
    "choice_class = 7\n",
    "choice_pic = 2\n",
    "\n",
    "train_original, shape_original = get_data_set(\"cifar10\", [choice_class], is_augment=False, downsample_percent=0.1)\n",
    "outil.plot_one_object(train_original[choice_pic], \"save_me_original\", \"./\")\n",
    "\n",
    "train_augmented, shape_augmented = get_data_set(\"cifar10\", [choice_class], is_augment=True, downsample_percent=0.1)\n",
    "outil.plot_one_object(train_augmented[len(train_original)+choice_pic], \"save_me_augmented\", \"./\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the GAN training\n",
    "\"\"\"\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from onenow_gan_trainer import GAN_TRAINER\n",
    "\n",
    "\n",
    "def run_train(x_train, object_shape, architecture_choice, optimizer_type, learning_rate, net_config, logger,\n",
    "              transfer_D_path, transfer_GD_path, D_num_frozen_layers, GD_num_frozen_layers, is_run_train = True):\n",
    "    # config\n",
    "    wandb.init(project=net_config['project_name'])\n",
    "    wandb.config.update(net_config)\n",
    "    # transfer\n",
    "    G_input_n = 100\n",
    "    A_net, discriminator_model, generator_model  = \\\n",
    "            get_adversarial_net(architecture_choice, optimizer_type, learning_rate, \n",
    "                                object_shape, G_input_n, net_config,\n",
    "                                transfer_D_path, transfer_GD_path,\n",
    "                                D_num_frozen_layers, GD_num_frozen_layers)\n",
    "    # train\n",
    "    trainer = GAN_TRAINER(x_train, A_net, net_config, logger) \n",
    "    if is_run_train:\n",
    "        trainer.train(net_config['train_batch_size'], G_input_n)   \n",
    "    return A_net    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_name=gan-cifar10-012345678-allfreeze__v023\tproject_tag=__data_set=cifar10__class_list=012345678__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024\n",
      "CREATION_FAILED=/home/ec2-user/SageMaker/efs/creation/gan-cifar10-012345678-allfreeze__v023__data_set=cifar10__class_list=012345678__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024 failed\n",
      "CREATION_FAILED=/home/ec2-user/SageMaker/efs/model/gan-cifar10-012345678-allfreeze__v023__data_set=cifar10__class_list=012345678__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024 failed\n",
      "x_train_before_class_filter= (50000, 32, 32, 3)\n",
      "x_train_after_downsample= (4500, 32, 32, 3)\n",
      "x_train_after_augmentation= (4500, 32, 32, 3)\n",
      "x_train_after_class_filter= (4500, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAJORITY CLASS PART1: Train CIFAR10 GAN Generator for all majority classes\n",
    "\"\"\"\n",
    "from onenow_gan_config import SystemConfig\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Search for best model by iterating over hyper-parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # init\n",
    "    version = \"23\"\n",
    "    name = \"gan-cifar10-012345678-allfreeze\"\n",
    "    project_name = (name + \"__v%s\") % str(format(int(version), '03d'))\n",
    "    global_config = SystemConfig(project_name)\n",
    "    wandb_config = global_config.get_config()\n",
    "    logger = logging.getLogger()    \n",
    "\n",
    "    # define hyper param space\n",
    "    hyperparam_space = {}  \n",
    "    hyperparam_space['data_set'] = ['cifar10'] #  'mnist'    \n",
    "    hyperparam_space['class_list'] = [0,1,2,3,4,5,6,7,8] \n",
    "    hyperparam_space['architecture_choice'] = ['brownlee'] #  'atienza', 'atiber'\n",
    "    hyperparam_space['optimizer_type'] = ['Adam']  # ', 'RMSprop'\n",
    "    hyperparam_space['learning_rate'] = [0.0002]  # 0.01*10**(-2), 0.1*10**(-2), 0.001*10**(-2)\n",
    "    hyperparam_space['batch_size'] = [1024] # 128, 4096, 8192, 1024, 256, \n",
    "    \n",
    "    \n",
    "    # iterate over hyper param space    \n",
    "    for dataset_name in hyperparam_space['data_set']:\n",
    "        for arch_choice in hyperparam_space['architecture_choice']:\n",
    "            wandb_config['architecture_choice'] = arch_choice\n",
    "            for optimizer_type in hyperparam_space['optimizer_type']: \n",
    "                wandb_config['optimizer_type'] = optimizer_type\n",
    "                for learning_rate in hyperparam_space['learning_rate']:\n",
    "                    wandb_config['learning_rate'] = learning_rate\n",
    "                    for train_batch_size in hyperparam_space['batch_size']:\n",
    "                        wandb_config['train_batch_size'] = train_batch_size\n",
    "                        ## path   \n",
    "                        project_tag = '__data_set=' + dataset_name\n",
    "                        class_string = \"\"\n",
    "                        for k in sorted(hyperparam_space['class_list']):\n",
    "                            class_string += str(k) \n",
    "                        project_tag += '__class_list=' + class_string\n",
    "                        project_tag += '__architecture=' + arch_choice \n",
    "                        project_tag += '__optimizer=' + optimizer_type \n",
    "                        project_tag += '__learningrate='+ str(learning_rate) \n",
    "                        project_tag += '__batchsize=' + str(train_batch_size) \n",
    "                        wandb_config['project_tag'] = project_tag\n",
    "                        print(\"project_name=\" + project_name + \"\\t\"+ \"project_tag=\" + project_tag)\n",
    "                        ## file\n",
    "                        global_config.set_log_file(logger, project_tag)\n",
    "                        global_config.set_creation_folder(project_tag)\n",
    "                        global_config.set_model_folder(project_tag)\n",
    "                        ## data set\n",
    "                        x_train, object_shape = get_data_set(dataset_name, hyperparam_space['class_list'],\n",
    "                                                             is_augment = False, downsample_percent=0.1) \n",
    "                        ## transfer learning\n",
    "                        # transfer_D_path = \"\"\n",
    "                        # transfer_GD_path = \"\"\n",
    "                        transfer_D_path = \"/home/ec2-user/SageMaker/Rosenblatt-AI/CS230/latest/discriminator.h5.latest.0-8\"\n",
    "                        transfer_GD_path = \"/home/ec2-user/SageMaker/Rosenblatt-AI/CS230/latest/generator.h5.latest.0-8\"\n",
    "                        # transfer_D_path = \"/home/ec2-user/SageMaker/efs/model/gan-cifar10-012345678__v019__data_set=cifar10__class_list=012345678__architecture=brownlee__optimizer=RMSprop__learningrate=0.0001__batchsize=1024/discriminator.h5.latest\"\n",
    "                        # transfer_GD_path = \"/home/ec2-user/SageMaker/efs/model/gan-cifar10-012345678__v019__data_set=cifar10__class_list=012345678__architecture=brownlee__optimizer=RMSprop__learningrate=0.0001__batchsize=1024/generator.h5.latest\"\n",
    "                        D_num_frozen_layers = 0\n",
    "                        GD_num_frozen_layers = 0\n",
    "                        ## run\n",
    "#                         try:\n",
    "#                             run_train(x_train, object_shape, \\\n",
    "#                                       arch_choice, optimizer_type, learning_rate, \\\n",
    "#                                       wandb_config, logger, \n",
    "#                                       transfer_D_path, transfer_GD_path,\n",
    "#                                       D_num_frozen_layers, GD_num_frozen_layers, True)\n",
    "#                         except Exception as e:\n",
    "#                             print(e)\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_name=gan-cifar10-class9-fromscratch-augmented10x__v025\tproject_tag=__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024\n",
      "CREATION_FAILED=/home/ec2-user/SageMaker/efs/creation/gan-cifar10-class9-fromscratch-augmented10x__v025__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024 failed\n",
      "CREATION_FAILED=/home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-fromscratch-augmented10x__v025__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024 failed\n",
      "x_train_before_class_filter= (50000, 32, 32, 3)\n",
      "x_train_after_downsample= (500, 32, 32, 3)\n",
      "x_train_after_augmentation= (5500, 32, 32, 3)\n",
      "x_train_after_class_filter= (5500, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAJORITY CLASS PART2: Enhance CIFAR10 Generator for all majority classes, performing Data Augmentation \n",
    "\"\"\"\n",
    "\n",
    "version = \"25\"\n",
    "name = \"gan-cifar10-class9-fromscratch-augmented10x\"\n",
    "project_name = (name + \"__v%s\") % str(format(int(version), '03d'))\n",
    "global_config = SystemConfig(project_name)\n",
    "wandb_config = global_config.get_config()\n",
    "logger = logging.getLogger()    \n",
    "  \n",
    "# define hyper param space\n",
    "hyperparam_space = {}  \n",
    "hyperparam_space['data_set'] = ['cifar10'] #  'mnist'    \n",
    "hyperparam_space['class_list'] = [9] #  0,1,2,3,4,5,6,7,8\n",
    "hyperparam_space['architecture_choice'] = ['brownlee'] #  'atienza', 'atiber', 'brownlee_truck'\n",
    "hyperparam_space['optimizer_type'] = ['Adam']  # ', 'RMSprop'\n",
    "hyperparam_space['learning_rate'] = [0.0002]  # 0.01*10**(-2), 0.1*10**(-2), 0.001*10**(-2)\n",
    "hyperparam_space['batch_size'] = [1024] # 128, 4096, 8192, 1024, 256, \n",
    "\n",
    "\n",
    "# iterate over hyper param space    \n",
    "for dataset_name in hyperparam_space['data_set']:\n",
    "    for arch_choice in hyperparam_space['architecture_choice']:\n",
    "        wandb_config['architecture_choice'] = arch_choice\n",
    "        for optimizer_type in hyperparam_space['optimizer_type']: \n",
    "            wandb_config['optimizer_type'] = optimizer_type\n",
    "            for learning_rate in hyperparam_space['learning_rate']:\n",
    "                wandb_config['learning_rate'] = learning_rate\n",
    "                for train_batch_size in hyperparam_space['batch_size']:\n",
    "                    wandb_config['train_batch_size'] = train_batch_size\n",
    "                    ## path   \n",
    "                    project_tag = '__data_set=' + dataset_name\n",
    "                    class_string = \"\"\n",
    "                    for k in sorted(hyperparam_space['class_list']):\n",
    "                        class_string += str(k) \n",
    "                    project_tag += '__class_list=' + class_string\n",
    "                    project_tag += '__architecture=' + arch_choice \n",
    "                    project_tag += '__optimizer=' + optimizer_type \n",
    "                    project_tag += '__learningrate='+ str(learning_rate) \n",
    "                    project_tag += '__batchsize=' + str(train_batch_size) \n",
    "                    wandb_config['project_tag'] = project_tag\n",
    "                    print(\"project_name=\" + project_name + \"\\t\"+ \"project_tag=\" + project_tag)\n",
    "                    ## file\n",
    "                    global_config.set_log_file(logger, project_tag)\n",
    "                    global_config.set_creation_folder(project_tag)\n",
    "                    global_config.set_model_folder(project_tag)\n",
    "                    ## data set\n",
    "                    x_train, object_shape = get_data_set(dataset_name, hyperparam_space['class_list'],\n",
    "                                                        is_augment = True, downsample_percent=0.1) \n",
    "                    ## transfer learning\n",
    "                    transfer_D_path = \"\"\n",
    "                    transfer_GD_path = \"\"\n",
    "                    # transfer_D_path = \"/home/ec2-user/SageMaker/Rosenblatt-AI/CS230/latest/discriminator.h5.1575816109.incomplete.9\"\n",
    "                    # transfer_GD_path = \"/home/ec2-user/SageMaker/Rosenblatt-AI/CS230/latest/generator.h5.1575816096.incomplete.9\"\n",
    "                    D_num_frozen_layers = 0\n",
    "                    GD_num_frozen_layers = 0 # 6\n",
    "                    ## run\n",
    "#                     try:\n",
    "#                         run_train(x_train, object_shape, \\\n",
    "#                                   arch_choice, optimizer_type, learning_rate, \\\n",
    "#                                   wandb_config, logger, \n",
    "#                                   transfer_D_path, transfer_GD_path,\n",
    "#                                   D_num_frozen_layers, GD_num_frozen_layers, True)\n",
    "#                     except Exception as e:\n",
    "#                         print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keras Nested Models Bug\n",
    "\"\"\"\n",
    "# https://github.com/keras-team/keras/pull/11847\n",
    "# https://github.com/tensorflow/tensorflow/issues/27769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_name=gan-cifar10-class9-trucky__v024\tproject_tag=__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024\n",
      "CREATION_FAILED=/home/ec2-user/SageMaker/efs/creation/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024 failed\n",
      "CREATION_FAILED=/home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024 failed\n",
      "x_train_before_class_filter= (50000, 32, 32, 3)\n",
      "x_train_after_downsample= (500, 32, 32, 3)\n",
      "x_train_after_augmentation= (5500, 32, 32, 3)\n",
      "x_train_after_class_filter= (5500, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/pablo_tech/gan-cifar10-class9-trucky__v024\" target=\"_blank\">https://app.wandb.ai/pablo_tech/gan-cifar10-class9-trucky__v024</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/pablo_tech/gan-cifar10-class9-trucky__v024/runs/kzwfpr18\" target=\"_blank\">https://app.wandb.ai/pablo_tech/gan-cifar10-class9-trucky__v024/runs/kzwfpr18</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " NETWORK_NAME= D_\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_42 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 522,497\n",
      "Trainable params: 522,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "at_step 0 TRANSFER_LEARNING_FROM= /home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/discriminator.h5.latest\n",
      "at_step 0 loaded_model= /home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/discriminator.h5.latest \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " NETWORK_NAME= GD_\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 4096)              413696    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DT (None, 8, 8, 128)         524416    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DT (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 32, 32, 3)         3459      \n",
      "=================================================================\n",
      "Total params: 1,466,115\n",
      "Trainable params: 1,466,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_42 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 522,497\n",
      "Trainable params: 522,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "at_step 0 TRANSFER_LEARNING_FROM= /home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/generator.h5.latest\n",
      "axes don't match array\n",
      "at_step 0 unable_to_load_model= /home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/generator.h5.latest \n",
      "\n",
      "\n",
      "\n",
      "<keras.layers.core.Dense object at 0x7effaa6f52b0> IS_TRAINABLE= False\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x7effaa725080> IS_TRAINABLE= False\n",
      "<keras.layers.core.Reshape object at 0x7effaa725240> IS_TRAINABLE= False\n",
      "<keras.layers.convolutional.Conv2DTranspose object at 0x7effaa7255c0> IS_TRAINABLE= False\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x7effaa6fe0f0> IS_TRAINABLE= False\n",
      "<keras.layers.convolutional.Conv2DTranspose object at 0x7effaa727b70> IS_TRAINABLE= False\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x7effaa727160> IS_TRAINABLE= False\n",
      "<keras.layers.convolutional.Conv2DTranspose object at 0x7effaa7279b0> IS_TRAINABLE= False\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x7effb0c471d0> IS_TRAINABLE= False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7effb0c47278> IS_TRAINABLE= False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EVALUATE GENERATOR: \n",
    "\"\"\"\n",
    "\n",
    "version = \"24\"\n",
    "name = \"gan-cifar10-class9-trucky\"\n",
    "project_name = (name + \"__v%s\") % str(format(int(version), '03d'))\n",
    "global_config = SystemConfig(project_name)\n",
    "wandb_config = global_config.get_config()\n",
    "logger = logging.getLogger()    \n",
    "  \n",
    "# define hyper param space\n",
    "hyperparam_space = {}  \n",
    "hyperparam_space['data_set'] = ['cifar10'] #  'mnist'    \n",
    "hyperparam_space['class_list'] = [9] #  0,1,2,3,4,5,6,7,8\n",
    "hyperparam_space['architecture_choice'] = ['brownlee'] #  'atienza', 'atiber', 'brownlee_truck'\n",
    "hyperparam_space['optimizer_type'] = ['Adam']  # ', 'RMSprop'\n",
    "hyperparam_space['learning_rate'] = [0.0002]  # 0.01*10**(-2), 0.1*10**(-2), 0.001*10**(-2)\n",
    "hyperparam_space['batch_size'] = [1024] # 128, 4096, 8192, 1024, 256, \n",
    "\n",
    "\n",
    "# iterate over hyper param space    \n",
    "for dataset_name in hyperparam_space['data_set']:\n",
    "    for arch_choice in hyperparam_space['architecture_choice']:\n",
    "        wandb_config['architecture_choice'] = arch_choice\n",
    "        for optimizer_type in hyperparam_space['optimizer_type']: \n",
    "            wandb_config['optimizer_type'] = optimizer_type\n",
    "            for learning_rate in hyperparam_space['learning_rate']:\n",
    "                wandb_config['learning_rate'] = learning_rate\n",
    "                for train_batch_size in hyperparam_space['batch_size']:\n",
    "                    wandb_config['train_batch_size'] = train_batch_size\n",
    "                    ## path   \n",
    "                    project_tag = '__data_set=' + dataset_name\n",
    "                    class_string = \"\"\n",
    "                    for k in sorted(hyperparam_space['class_list']):\n",
    "                        class_string += str(k) \n",
    "                    project_tag += '__class_list=' + class_string\n",
    "                    project_tag += '__architecture=' + arch_choice \n",
    "                    project_tag += '__optimizer=' + optimizer_type \n",
    "                    project_tag += '__learningrate='+ str(learning_rate) \n",
    "                    project_tag += '__batchsize=' + str(train_batch_size) \n",
    "                    wandb_config['project_tag'] = project_tag\n",
    "                    print(\"project_name=\" + project_name + \"\\t\"+ \"project_tag=\" + project_tag)\n",
    "                    ## file\n",
    "                    global_config.set_log_file(logger, project_tag)\n",
    "                    global_config.set_creation_folder(project_tag)\n",
    "                    global_config.set_model_folder(project_tag)\n",
    "                    ## data set\n",
    "                    x_train, object_shape = get_data_set(dataset_name, hyperparam_space['class_list'],\n",
    "                                                        is_augment = True, downsample_percent=0.1) \n",
    "                    ## transfer learning\n",
    "                    transfer_D_path = \"\"\n",
    "                    transfer_GD_path = \"\"\n",
    "#                     transfer_D_path = \"/home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/discriminator.h5.latest\"\n",
    "#                     transfer_GD_path = \"/home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/generator.h5.latest\"                    \n",
    "#                     transfer_D_path = \"/home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/discriminator.h5.1575766537\"\n",
    "#                     transfer_GD_path = \"/home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-trucky__v024__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/generator.h5.1575766534\"\n",
    "                    D_num_frozen_layers = 0\n",
    "                    GD_num_frozen_layers = 20\n",
    "                    ## run\n",
    "                    try:\n",
    "                        g_a_n_trucky = \\\n",
    "                            run_train(x_train, object_shape, \\\n",
    "                                      arch_choice, optimizer_type, learning_rate, \\\n",
    "                                      wandb_config, logger, \n",
    "                                      transfer_D_path, transfer_GD_path,\n",
    "                                      D_num_frozen_layers, GD_num_frozen_layers, False)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_name=gan-cifar10-class9-fromscratch-augmented10x__v025\tproject_tag=__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024\n",
      "CREATION_FAILED=/home/ec2-user/SageMaker/efs/creation/gan-cifar10-class9-fromscratch-augmented10x__v025__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024 failed\n",
      "CREATION_FAILED=/home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-fromscratch-augmented10x__v025__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024 failed\n",
      "x_train_before_class_filter= (50000, 32, 32, 3)\n",
      "x_train_after_downsample= (500, 32, 32, 3)\n",
      "x_train_after_augmentation= (5500, 32, 32, 3)\n",
      "x_train_after_class_filter= (5500, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/pablo_tech/gan-cifar10-class9-fromscratch-augmented10x__v025\" target=\"_blank\">https://app.wandb.ai/pablo_tech/gan-cifar10-class9-fromscratch-augmented10x__v025</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/pablo_tech/gan-cifar10-class9-fromscratch-augmented10x__v025/runs/y78yzti5\" target=\"_blank\">https://app.wandb.ai/pablo_tech/gan-cifar10-class9-fromscratch-augmented10x__v025/runs/y78yzti5</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " NETWORK_NAME= D_\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_47 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 522,497\n",
      "Trainable params: 522,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "at_step 0 TRANSFER_LEARNING_FROM= /home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-fromscratch-augmented10x__v025__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/discriminator.h5.latest\n",
      "at_step 0 loaded_model= /home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-fromscratch-augmented10x__v025__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/discriminator.h5.latest \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " NETWORK_NAME= GD_\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 4096)              413696    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DT (None, 8, 8, 128)         524416    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DT (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_30 (Conv2DT (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 32, 32, 3)         3459      \n",
      "=================================================================\n",
      "Total params: 1,466,115\n",
      "Trainable params: 1,466,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_47 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 522,497\n",
      "Trainable params: 522,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "at_step 0 TRANSFER_LEARNING_FROM= /home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-fromscratch-augmented10x__v025__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/generator.h5.latest\n",
      "at_step 0 loaded_model= /home/ec2-user/SageMaker/efs/model/gan-cifar10-class9-fromscratch-augmented10x__v025__data_set=cifar10__class_list=9__architecture=brownlee__optimizer=Adam__learningrate=0.0002__batchsize=1024/generator.h5.latest \n",
      "\n",
      "\n",
      "\n",
      "<keras.layers.core.Dense object at 0x7effa91580f0> IS_TRAINABLE= True\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x7effa9158fd0> IS_TRAINABLE= True\n",
      "<keras.layers.core.Reshape object at 0x7effa9158c50> IS_TRAINABLE= True\n",
      "<keras.layers.convolutional.Conv2DTranspose object at 0x7effa9158f98> IS_TRAINABLE= True\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x7effa9430438> IS_TRAINABLE= True\n",
      "<keras.layers.convolutional.Conv2DTranspose object at 0x7effa9430128> IS_TRAINABLE= True\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x7effa92b03c8> IS_TRAINABLE= True\n",
      "<keras.layers.convolutional.Conv2DTranspose object at 0x7effa946ceb8> IS_TRAINABLE= True\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x7effa914a438> IS_TRAINABLE= True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7effa914a2e8> IS_TRAINABLE= True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EVALUATE GENERATOR: \n",
    "\"\"\"\n",
    "\n",
    "version = \"25\"\n",
    "name = \"gan-cifar10-class9-fromscratch-augmented10x\"\n",
    "project_name = (name + \"__v%s\") % str(format(int(version), '03d'))\n",
    "global_config = SystemConfig(project_name)\n",
    "wandb_config = global_config.get_config()\n",
    "logger = logging.getLogger()    \n",
    "  \n",
    "# define hyper param space\n",
    "hyperparam_space = {}  \n",
    "hyperparam_space['data_set'] = ['cifar10'] #  'mnist'    \n",
    "hyperparam_space['class_list'] = [9] #  0,1,2,3,4,5,6,7,8\n",
    "hyperparam_space['architecture_choice'] = ['brownlee'] #  'atienza', 'atiber', 'brownlee_truck'\n",
    "hyperparam_space['optimizer_type'] = ['Adam']  # ', 'RMSprop'\n",
    "hyperparam_space['learning_rate'] = [0.0002]  # 0.01*10**(-2), 0.1*10**(-2), 0.001*10**(-2)\n",
    "hyperparam_space['batch_size'] = [1024] # 128, 4096, 8192, 1024, 256, \n",
    "\n",
    "\n",
    "# iterate over hyper param space    \n",
    "for dataset_name in hyperparam_space['data_set']:\n",
    "    for arch_choice in hyperparam_space['architecture_choice']:\n",
    "        wandb_config['architecture_choice'] = arch_choice\n",
    "        for optimizer_type in hyperparam_space['optimizer_type']: \n",
    "            wandb_config['optimizer_type'] = optimizer_type\n",
    "            for learning_rate in hyperparam_space['learning_rate']:\n",
    "                wandb_config['learning_rate'] = learning_rate\n",
    "                for train_batch_size in hyperparam_space['batch_size']:\n",
    "                    wandb_config['train_batch_size'] = train_batch_size\n",
    "                    ## path   \n",
    "                    project_tag = '__data_set=' + dataset_name\n",
    "                    class_string = \"\"\n",
    "                    for k in sorted(hyperparam_space['class_list']):\n",
    "                        class_string += str(k) \n",
    "                    project_tag += '__class_list=' + class_string\n",
    "                    project_tag += '__architecture=' + arch_choice \n",
    "                    project_tag += '__optimizer=' + optimizer_type \n",
    "                    project_tag += '__learningrate='+ str(learning_rate) \n",
    "                    project_tag += '__batchsize=' + str(train_batch_size) \n",
    "                    wandb_config['project_tag'] = project_tag\n",
    "                    print(\"project_name=\" + project_name + \"\\t\"+ \"project_tag=\" + project_tag)\n",
    "                    ## file\n",
    "                    global_config.set_log_file(logger, project_tag)\n",
    "                    global_config.set_creation_folder(project_tag)\n",
    "                    global_config.set_model_folder(project_tag)\n",
    "                    ## data set\n",
    "                    x_train, object_shape = get_data_set(dataset_name, hyperparam_space['class_list'],\n",
    "                                                        is_augment = True, downsample_percent=0.1) \n",
    "                    ## transfer learning\n",
    "                    transfer_D_path = \"\"\n",
    "                    transfer_GD_path = \"\"\n",
    "                    D_num_frozen_layers = 0\n",
    "                    GD_num_frozen_layers = 0 # 6\n",
    "                    ## run\n",
    "                    try:\n",
    "                        g_a_n_10x = \\\n",
    "                            run_train(x_train, object_shape, \\\n",
    "                                      arch_choice, optimizer_type, learning_rate, \\\n",
    "                                      wandb_config, logger, \n",
    "                                      transfer_D_path, transfer_GD_path,\n",
    "                                      D_num_frozen_layers, GD_num_frozen_layers, False)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d820d43405cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4500\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mG_input_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mz_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_input_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mG_of_z_fake_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_a_n_10x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_G_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\"\"\"\n",
    "10x Augmented Generation\n",
    "\"\"\"\n",
    "num_samples = 4500 * 50\n",
    "G_input_n = 100\n",
    "z_noise = np.random.uniform(-1.0, 1.0, size=[num_samples, G_input_n])\n",
    "G_of_z_fake_objects = g_a_n_10x.get_G_model().predict(z_noise)\n",
    "\n",
    "denormalized_fake = (G_of_z_fake_objects + 1.0) / 2.0\n",
    "\n",
    "to_plot = denormalized_fake[0]\n",
    "\n",
    "outil.plot_one_object(to_plot, \"save_me_fake.png\", \"./\")\n",
    "\n",
    "print(\"SIZE_OF_FAKE=\", G_of_z_fake_objects.shape, to_plot.shape)\n",
    "# print(G_of_z_fake_objects)\n",
    "\n",
    "np.save('/home/ec2-user/SageMaker/efs/generated_examples_class_9.png', denormalized_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(denormalized_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
